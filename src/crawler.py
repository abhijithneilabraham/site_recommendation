import requests from bs4 import BeautifulSoup import operator import refrom collections import Counter from nltk.stem import WordNetLemmatizer from nltk.corpus import stopwords,wordnetimport jsonimport osimport randomstop_words = set(stopwords.words('english'))syns = wordnet.synsetslemmatizer = WordNetLemmatizer()lem=lemmatizer.lemmatizeclass Crawler():      def del_dump(self,filepath): #if json dump exists, remove the dump        if os.path.isfile(filepath):            os.remove(filepath)                def create_json_dump(self,filepath,data): #create a json dump with given data mappings        self.del_dump(filepath)        with open(filepath, "w") as write_file:            json.dump(data, write_file)                    # Function removes any unwanted symbols     def clean_wordlist(self,wordlist):         clean_list =[]         for word in wordlist:             symbols = '!@#$%^&*()_-+={[}]|\;:"<>?/., '                        for i in range (0, len(symbols)):                 word = word.replace(symbols[i], '')             if len(word) > 0:                 clean_list.append(word)             return list(set(clean_list))        def get_links(self,soup,high_level_url): #get sub urls of a given url        links={high_level_url:[]}        for link in soup.find_all(attrs={'href': re.compile("http")}):            links[high_level_url].append(link.get('href'))        return links            def get_keywords(self,url): #get keywords from a given url        wordlist = []             source_code = requests.get(url).text         soup = BeautifulSoup(source_code, 'html.parser')         for each_text in soup.findAll():             content = each_text.text             words = content.lower().split()             words=[lem(i) for i in words if i not in stop_words and i.isalnum()] #lemmatizing, removing stopwords and cleaning non alphanumeric characters            for each_word in words:                 wordlist.append(each_word)         site_keywords=self.clean_wordlist(wordlist)         return site_keywords    def get_traffic(self,url): #random number generator to get traffic on a website        return random.randint(0, 100)        def get_synonyms(self,word): #get synonyms for a word to add to the list of words        synobj=syns(word)        synonyms=list(set([i.lemmas()[0].name().lower() for i in synobj]))        return synonyms        def crawl(self,url): #crawl a given url and its sub urls and get keywords        source_code = requests.get(url).text         soup = BeautifulSoup(source_code, 'html.parser')         url_maps=self.get_links(soup, url)        keywords=self.get_keywords(url)        for sub_urls in url_maps.values():            for sub_url in sub_urls:                print(sub_url)                try:                    keywords+=self.get_keywords(sub_url)                except not KeyboardInterrupt:                    continue        bulk_keywords=[]        for kwd in keywords:            bulk_keywords+=self.get_synonyms(kwd)             bulk_keywords=list(set(bulk_keywords))        return keywords                